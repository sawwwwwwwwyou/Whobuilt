[
  {
    "id": 9,
    "slug": "fallas-valencia-2026-survival-guide",
    "tag": "TRAVEL",
    "date": "FEB 24",
    "title": "Fallas Valencia 2026 Survival Guide: 15 Tips You Actually Need",
    "domain": "whobuilt.app",
    "excerpt": "Las Fallas is five days of controlled chaos across an entire city — 750+ burning monuments, daily gunpowder explosions, 2M visitors. This guide covers the practical things that will make or break your experience.",
    "votes": 2140,
    "comments": 230,
    "readTime": 8,
    "featured": true,
    "snippets": [
      {
        "lang": "english",
        "code": "WHAT TO PACK:\n\n1. Earplugs — La Mascleta is painfully loud\n2. Comfortable walking shoes — 12-18 km/day\n3. Portable battery pack — your phone will die\n4. Light layers — 12-20°C in March\n5. Small daypack — water, battery, snacks\n\nKEY DATES:\n15-19 March: Main festival\n18 March: La Ofrenda (flower procession)\n19 March: La Crema (all monuments burn)"
      },
      {
        "lang": "español",
        "code": "QUÉ LLEVAR:\n\n1. Tapones para los oídos — La Mascleta duele\n2. Calzado cómodo — 12-18 km al día\n3. Batería portátil — tu móvil va a morir\n4. Capas ligeras — 12-20°C en marzo\n5. Mochila pequeña — agua, batería, comida\n\nFECHAS CLAVE:\n15-19 marzo: Festival principal\n18 marzo: La Ofrenda (procesión de flores)\n19 marzo: La Crema (se queman todos los monumentos)"
      }
    ]
  },
  {
    "id": 10,
    "slug": "best-apps-for-las-fallas-2026",
    "tag": "TRAVEL",
    "date": "FEB 24",
    "title": "Best Apps for Las Fallas 2026: Which One Should You Download?",
    "domain": "whobuilt.app",
    "excerpt": "Over 750 fallas monuments, a city-wide festival, and millions of visitors. You need the right app. This guide compares your realistic options and explains which Fallas map app is worth installing.",
    "votes": 1380,
    "comments": 95,
    "readTime": 5,
    "featured": false,
    "snippets": [
      {
        "lang": "english",
        "code": "MIS FALLAS APP — WHAT IT COVERS:\n\n✓ 750+ monuments on interactive Mapbox map\n✓ 8 category filters (Especial, Infantil, etc.)\n✓ Full event calendar grouped by date\n✓ Walking directions to any monument\n✓ Works offline (cached map tiles)\n✓ No login required\n✓ EN / ES / Valencian\n✓ Free on iOS"
      },
      {
        "lang": "español",
        "code": "APP MIS FALLAS — QUÉ INCLUYE:\n\n✓ 750+ monumentos en mapa Mapbox interactivo\n✓ 8 filtros de categoría (Especial, Infantil...)\n✓ Calendario completo de eventos por fecha\n✓ Rutas a pie hasta cualquier monumento\n✓ Funciona sin cobertura (tiles cacheados)\n✓ Sin registro necesario\n✓ ES / EN / Valenciano\n✓ Gratis en iOS"
      }
    ]
  },
  {
    "id": 11,
    "slug": "dog-vibe-codes-games-with-ai",
    "tag": "AI",
    "date": "FEB 25",
    "title": "This Developer's Dog Is Now a Game Dev (Sort Of)",
    "domain": "whobuilt.app",
    "excerpt": "A dev rigged up an AI vibe coding setup where their dog presses buttons and the AI generates actual game logic. It hit 1042 upvotes on HN — turns out the barrier to game dev is now literally paw-level.",
    "votes": 1042,
    "comments": 335,
    "readTime": 3,
    "featured": false,
    "body": "Caleb Leak's dog doesn't write code. But his dog does generate it — and that distinction is doing a lot of heavy lifting right now in a few corners of the internet.\n\nThe setup is simple enough to be absurd and clever enough to be genuinely interesting: Leak wired up pressure sensors and buttons around his home that his dog interacts with — sitting on them, pawing at them, walking across a mat. Those physical inputs feed into a vibe coding pipeline where an AI interprets the signal patterns and produces actual game logic. Not pseudocode. Not suggestions. Functional game behavior that gets integrated into a real project. No human writes a single line of traditional code. The dog walks across the mat, the AI decides that means \"spawn an enemy,\" and it ships.\n\nThe Hacker News thread hit 1042 upvotes, and the comments predictably split into two camps. One side called it a stunt — and fair enough, it is a stunt. The other side got uncomfortable in a more productive way, asking questions that didn't have clean answers: *Is this programming? Is Caleb the programmer? Is the dog?* If you define programming as \"authoring instructions that a machine executes,\" then the dog is closer to a programmer than most people are comfortable admitting. The AI is just the compiler.\n\nWhat's actually being stress-tested here is the assumption that programming requires intent. Caleb had intent — he built the system, chose what the inputs would map to, curated the output. His dog had no intent beyond whatever dogs have when they sit on things. But the game logic that came out is as valid as anything you'd write manually. The authorship question gets weird fast. You could argue Caleb is the programmer and the dog is a random number generator with fur. You could also argue the AI is the programmer and both Caleb and the dog are just providing it with prompts of varying coherence.\n\nThe broader implication isn't really about dogs. It's about what \"vibe coding\" is actually doing to the definition of software authorship. Tools like Cursor, Devin, and similar AI coding assistants have already pushed the human role toward something more like direction and curation than construction. Leak just took that to its logical extreme by removing the human from the input loop entirely and replacing them with an animal. The result still compiles. The result still runs. The result is, by most practical definitions, software.\n\nWhether that bothers you probably depends on how much of your identity is tied to the craft of writing code versus the outcome of shipping working software. If you're in the second camp, a dog making games is just a funny story. If you're in the first, it's a slightly uncomfortable mirror.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- Developer built a physical input setup letting his dog trigger AI game generation\n- The AI interprets button/sensor presses and produces functional game logic\n- No traditional code written — the dog is a legitimate co-creator\n- Went viral on HN with 1042 points, sparking debate about what 'programming' means\n- Vibe coding has pushed human role from construction to direction and curation\n- The real story: AI has removed humans from the input loop — pets optional"
      }
    ]
  },
  {
    "id": 12,
    "slug": "denmark-ditches-microsoft-open-source",
    "tag": "OPEN SOURCE",
    "date": "FEB 25",
    "title": "Denmark's Government Is Done with Microsoft",
    "domain": "whobuilt.app",
    "excerpt": "Denmark's Agency for Digital Government is moving off Microsoft software in favor of open-source alternatives, citing digital sovereignty and cost. It's one of the largest public-sector Microsoft exits in Europe.",
    "votes": 498,
    "comments": 271,
    "readTime": 4,
    "featured": false,
    "body": "Denmark just drew a line in the sand, and it's aimed squarely at Redmond.\n\nThe Danish Agency for Digital Government has announced it's moving away from Microsoft — Office, cloud services, collaboration tools, the whole stack — in favor of open-source alternatives. The stated motivations are digital sovereignty and cost reduction. The practical motivation, reading between the lines, is that a US administration with unpredictable policy positions and demonstrated willingness to use tech platforms as geopolitical leverage is no longer a comfortable landlord for a European government's critical infrastructure.\n\nThis isn't Denmark going rogue. It's Denmark joining a pattern. Germany's Schleswig-Holstein state has been executing a similar migration since 2021, explicitly citing independence from US and Chinese providers. France's Gendarmerie famously moved 90,000 desktops to Linux over a decade ago and saved an estimated €2 million annually compared to staying on Windows. The EU has its own ongoing push toward digital sovereignty through initiatives like Gaia-X. Denmark's announcement is the latest data point in what's becoming a structural shift in how European public institutions think about software dependencies.\n\nThe technical challenge here is real and shouldn't be hand-waved. Microsoft's suite — Exchange, Teams, SharePoint, OneDrive, Azure AD — is deeply integrated in most government operations. Swapping it out isn't a weekend project. LibreOffice handles documents adequately. Nextcloud handles file storage adequately. Matrix or Element handles secure messaging adequately. But \"adequately\" is doing a lot of work in those sentences, and enterprise-grade migration at government scale means thousands of users, legacy file formats, existing workflows, and procurement processes that move at geological speed.\n\nWhat's notable about Denmark's announcement is the lack of a hard deadline. That's either pragmatic — acknowledging that rushing the migration would create chaos — or it's political cover that lets future administrations quietly de-prioritize it. The difference between a firm strategic direction and an announcement that gets walked back in three years is implementation commitment, and that's still TBD.\n\nFor developers and IT teams in European public sector contexts, this signals something worth paying attention to: procurement decisions are increasingly going to favor vendors who can demonstrate data residency, open standards compliance, and — critically — who are incorporated somewhere that isn't subject to CLOUD Act jurisdiction. Interoperability with open formats, self-hostability, and auditable codebases are moving from nice-to-have to table stakes.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- Denmark's Agency for Digital Government officially plans to exit Microsoft ecosystem\n- Primary drivers: digital sovereignty + long-term cost reduction\n- Joins Germany (Schleswig-Holstein) and France (Gendarmerie) in similar moves\n- Full stack replacement: Office, cloud, collaboration tools → open-source\n- No hard deadline — direction is firm, timeline is not\n- CLOUD Act jurisdiction concerns driving European public sector away from US vendors\n- Self-hostability and open standards becoming procurement requirements"
      }
    ]
  },
  {
    "id": 13,
    "slug": "claude-code-remote-control-api",
    "tag": "DEV",
    "date": "FEB 25",
    "title": "Claude Code Now Has a Remote Control API",
    "domain": "whobuilt.app",
    "excerpt": "Anthropic shipped remote control for Claude Code — developers can now drive Claude Code instances programmatically via API, enabling fully automated coding pipelines and agent-to-agent workflows. This turns Claude Code from a dev tool into an orchestratable coding engine.",
    "votes": 272,
    "comments": 165,
    "readTime": 3,
    "featured": false,
    "body": "Anthropic just made Claude Code significantly more interesting for anyone building automated development pipelines.\n\nThe short version: Claude Code now has a remote control API. Instead of only operating interactively through the CLI — where a human sits at a terminal, types requests, and reviews output — you can now drive Claude Code instances programmatically. You send it tasks via API, it executes them autonomously, and you get results back. The documentation lives at code.claude.com/docs/en/remote-control.\n\nThe use cases this unlocks are straightforward and immediately useful. The most obvious is CI/CD integration: instead of Claude Code being a tool you use during development, it becomes a step in your pipeline. A PR comes in, your CI system spins up a Claude Code instance, asks it to review the diff for security issues or test coverage gaps, and posts the results as a comment — no human in the loop. You've seen this pattern with static analysis tools for years. Claude Code is now pluggable into the same slots, but with the ability to reason about code rather than just pattern-match against rules.\n\nThe more interesting case is agent-to-agent orchestration. One AI system can now programmatically control Claude Code as a sub-agent. If you're building an autonomous development workflow — say, an orchestrator that breaks down a feature request, assigns subtasks, and coordinates execution across multiple specialized agents — Claude Code becomes a composable component rather than a standalone tool. Your orchestrator doesn't need to know how to write code. It just needs to know how to write a task description and hand it off.\n\nThis is worth thinking about carefully, because it changes the trust model. When a human drives Claude Code interactively, there's a review loop at every meaningful decision point. When another AI drives Claude Code programmatically, that loop is either automated or absent entirely. You're trusting the orchestrator's judgment about what to ask Claude Code to do, and you're trusting Claude Code's judgment about how to do it, and those errors can compound before a human sees the output. The controls you put around the remote API — what permissions the Claude Code instance has, what it can touch in your repo, how changes are staged before commit — matter a lot more in this mode.\n\nFrom a practical standpoint, the API enables patterns that developers have been hacking together manually for a while. People have been scripting Claude Code via CLI wrappers and expect-style automation for months. A proper API with documented behavior and stable interfaces is a significant upgrade over that. The broader trajectory is clear: AI coding tools are moving from interactive assistants to autonomous pipeline components.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- Claude Code now controllable via remote API (not just interactive CLI)\n- Enables CI/CD pipelines with Claude Code as an autonomous review/coding step\n- Agent-to-agent orchestration: one AI can drive Claude Code as a sub-agent\n- Trust model changes significantly — human review loop becomes optional\n- Replaces months of CLI wrapper hacks with a stable documented API\n- Documented at code.claude.com/docs/en/remote-control\n- AI coding tools moving from interactive assistants to pipeline components"
      }
    ]
  },
  {
    "id": 14,
    "slug": "karpathy-microgpt-200-lines-pure-python",
    "tag": "AI",
    "date": "MAR 01",
    "title": "Karpathy's MicroGPT: A Full LLM in 200 Lines of Python",
    "domain": "whobuilt.app",
    "excerpt": "Andrej Karpathy published a 200-line pure Python file that trains and runs a GPT with zero dependencies. No PyTorch, no NumPy. The entire stack -- autograd, architecture, tokenizer, training loop -- fits in a single file. Key insight: everything else is just efficiency.",
    "votes": 731,
    "comments": 120,
    "readTime": 3,
    "featured": false,
    "body": "Andrej Karpathy published a 200-line Python file that trains and runs a GPT. No PyTorch, no NumPy, no dependencies of any kind. Everything -- autograd, the neural network architecture, tokenization, the training loop, inference -- sits in a single file that fits on three columns of a page.\n\nThe file, available as a GitHub gist and on Google Colab, is the culmination of a series Karpathy has been building for years: micrograd (a minimal autograd engine), makemore (character-level language models), nanoGPT (a clean GPT-2 implementation). MicroGPT collapses the entire stack into one place. It trains on a dataset of 32,000 names and learns to generate plausible new ones. The training converges. The inference works. The whole thing runs in pure Python.\n\nThe core mechanism is a `Value` class that represents a scalar and tracks its gradient. Every arithmetic operation -- addition, multiplication, tanh activation -- records itself in a computation graph. When you call `backward()`, the engine walks that graph in reverse topological order and applies the chain rule at each node to compute gradients. This is autograd from scratch, identical in principle to what PyTorch does, without the C++ kernel and GPU dispatch underneath. The GPT-2-like architecture sits on top of that: attention heads, layer norms, embeddings -- all built from these scalar operations. Training uses a hand-written Adam optimizer. The whole thing is slow by any practical measure, but it runs.\n\nThe key insight Karpathy is demonstrating is captured in one sentence: **\"Everything else is just efficiency.\"** PyTorch, CUDA, flash attention, quantization, distributed training -- those are real and important, but they are optimizations on top of a core algorithm you can write in an afternoon. The algorithmic content of an LLM is surprisingly small. Production frameworks exist to run that algorithm fast at scale, not to make the algorithm possible.\n\nThis is also the fourth iteration of the same argument at increasing compression. Micrograd proved you could build autograd in roughly 150 lines. Makemore proved you could build a character LLM from scratch. NanoGPT proved you could train a serious GPT-2 in clean PyTorch. MicroGPT collapses all of that, removing PyTorch from the picture entirely. Each step in the series makes the there-is-no-magic-here argument harder to dismiss.\n\nFor you as a developer, the practical value is not that you should use microgpt for production work. It is that reading 200 lines gives you a ground-truth understanding of what your frameworks are actually doing. If you have been treating transformers as a black box and calling `model.fit()`, microgpt is the x-ray. When your training loss stops decreasing, or gradients explode, or attention patterns degenerate, you are much better equipped to diagnose it if you have once held the whole system in your head at once. The gist takes about an hour to read carefully. That hour has a high expected return.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- 200-line pure Python file, zero dependencies, full LLM in one gist\n- Implements autograd from scratch via a scalar Value class\n- Backprop via topological sort and chain rule, same as PyTorch under the hood\n- Trains on 32,000 names dataset; generates plausible new names at inference\n- GPT-2-like architecture: attention heads, layer norms, hand-written Adam optimizer\n- Core insight: algorithms are small; everything else is efficiency and scale\n- Culmination of micrograd, makemore, nanoGPT series"
      }
    ]
  },
  {
    "id": 15,
    "slug": "obsidian-sync-headless-client-servers",
    "tag": "DEV",
    "date": "MAR 01",
    "title": "Obsidian Sync Gets a Headless Client",
    "domain": "whobuilt.app",
    "excerpt": "Obsidian shipped a headless sync client -- you can now run Obsidian Sync as a background daemon on Linux servers, Raspberry Pis, and Docker containers. This closes a real gap for teams using Obsidian as a shared knowledge base on headless infrastructure.",
    "votes": 470,
    "comments": 160,
    "readTime": 3,
    "featured": false,
    "body": "Obsidian shipped a headless sync client. You can now run Obsidian Sync without opening the desktop application -- as a background daemon, inside a Docker container, on a Raspberry Pi, or anywhere else without a display.\n\nBefore this, maintaining a synced vault on a server was a problem without a clean solution. The official path was running the full Electron desktop app, which requires a display or a virtual framebuffer -- not something you want on a production server. The unofficial path was stitching together rclone, git hooks, or WebDAV adapters, and hoping Obsidian's internal sync format did not change between releases. For individuals, this was manageable friction. For teams running shared vaults as knowledge bases or internal wikis, it was a genuine infrastructure gap.\n\nThe headless client runs as `obsidian-sync` and implements the same sync protocol as the desktop app. Your vault stays current on remote machines continuously, without human involvement. The practical applications are specific: a documentation server rendering vault notes to a static site, a CI pipeline pulling shared docs before builds, an automated backup destination, or an always-on sync node on network-attached storage. The vault-as-writable-database pattern becomes realistic when you can guarantee the sync layer is always running.\n\nThe subscription requirement is real -- Obsidian Sync costs $8/month, and the headless client does not change that. The client is also not open-source, so if your goal is full ownership of sync infrastructure, this does not move the needle. Git-based workflows remain the right answer for that case. What the headless client addresses is a specific and common situation: you already use and trust Obsidian Sync for its conflict resolution and reliability, but you need a server-side participant in the sync graph and do not want to stand up a full desktop environment to get it.\n\nWhat is telling is the volume of DIY workarounds this announcement surfaced. Multiple GitHub repos named some variant of obsidian-headless-sync -- shell scripts, Electron renderer patches, rclone configs, undocumented API calls. These solutions shared a failure mode: Obsidian updates, something internal shifts, and your carefully maintained sync pipeline breaks silently, often discovered days later when someone notices the server vault is weeks behind. Official tooling with documented behavior has a different reliability profile than artisanal shell scripts.\n\nIf you already pay for Obsidian Sync and run any server infrastructure, the headless client is straightforward to justify. Run it on your backup server or NAS and let it stay current automatically. The comparison is not between headless and nothing -- it is between headless and whatever improvised sync script you are currently babysitting.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- Obsidian Sync now works without GUI -- runs as background daemon\n- Supports Linux servers, Raspberry Pi, Docker, CI pipelines, NAS\n- Same sync protocol as desktop app -- same conflict resolution behavior\n- Requires active Obsidian Sync subscription ($8/month)\n- Not open-source; git-based sync still better for full infrastructure ownership\n- Replaces fragile DIY rclone/git-hook workarounds with official tooling\n- Enables vault-as-database patterns for downstream automation"
      }
    ]
  },
  {
    "id": 16,
    "slug": "windows-95-ui-usability-engineering-1996",
    "tag": "DESIGN",
    "date": "MAR 01",
    "title": "The 1996 Windows 95 UI Case Study Still Holds Up",
    "domain": "whobuilt.app",
    "excerpt": "A 1996 ACM paper on the Windows 95 interface resurfaced on HN. Microsoft ran 85+ usability studies with thousands of participants to design what became the most widely shipped GUI in history. The Start button, taskbar, and program discovery -- all came from research, not intuition.",
    "votes": 253,
    "comments": 150,
    "readTime": 3,
    "featured": false,
    "body": "A 1996 ACM paper on the Windows 95 user interface resurfaced on Hacker News and pulled 250+ upvotes from people who work in UI professionally. \"The Windows 95 User Interface: A Case Study in Usability Engineering\" by Sullivan et al. describes how Microsoft ran more than 85 usability studies -- involving thousands of participants -- to design what became the most widely shipped GUI in history.\n\nThe methodology in the paper reads like a modern product handbook, except it was codified thirty years before that handbook existed. The team observed users with existing software to understand real behavior rather than assumed behavior. They built low-fidelity prototypes early and tested them before committing to implementation. They iterated based on test results rather than defending original designs. They measured task completion rates, error rates, and time-on-task systematically. This is standard UX practice in 2026. In 1993, when most of this work was happening, it was rigorous enough to be a conference paper.\n\nThe individual decisions documented in the paper reveal something counterintuitive: the UI patterns you consider obvious were not arrived at by reasoning from first principles. The Start button exists because research showed that users sitting in front of Windows 3.1 had no idea where to begin. They were not failing to find the right button -- they did not have a mental model for what starting a computer meant. The taskbar replaced Program Manager because observation of actual working behavior showed constant app-switching, and Program Manager required navigating back to a fixed home base every time. User testing found that a majority of participants could not locate programs they had already installed. Every one of these is a finding, not an assumption.\n\nThe HN discussion reveals how developers think about UI now. Several commenters noted they had assumed these patterns were obvious, or that Microsoft had simply made reasonable choices. The paper makes clear that none of it was obvious, and several reasonable-seeming choices turned out to be wrong under testing. The discussion also surfaced observations from people at large tech companies: current product teams do considerably less of this kind of research than Microsoft did in 1993, despite having far more data available.\n\n**What is worth taking from this is not nostalgia.** It is the specific methodology: observe actual behavior before designing, prototype before building, test with real users before shipping, and treat test results as authoritative rather than as obstacles to your existing plan. These are not controversial ideas. They are hard to execute when shipping fast, and the path of least resistance is to trust your own product intuition. The Windows 95 case study is a durable reminder that product intuition -- even at a company full of experienced people with deep domain knowledge -- is frequently wrong, and that the only reliable correction is contact with actual users.",
    "snippets": [
      {
        "lang": "english",
        "code": "KEY POINTS:\n\n- 1996 ACM paper: 85+ usability studies behind Windows 95 UI decisions\n- Start button came from research: users had no mental model of where to begin\n- Taskbar replaced Program Manager based on observed app-switching behavior\n- Majority of test users could not find programs they had already installed\n- Methodology: observe, prototype, test, iterate -- same as modern UX playbooks\n- HN: current product teams do less user research than 1993 Microsoft did\n- Obvious UI patterns were expensive research results, not intuitive design"
      }
    ]
  }
]